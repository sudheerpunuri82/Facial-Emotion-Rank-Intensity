# -*- coding: utf-8 -*-
"""Copy of LRP_IEEEAccess_Review Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13ITPa6YtTJb8gOVy9FyU5bgiDsQKLQFp
"""

from google.colab import drive
drive.mount('/content/drive')

# @title
# %% Imports
import torch
import torch.nn as nn
import torchvision
import torchvision.models as models
import torchvision.transforms as transforms
import numpy as np
import matplotlib.pyplot as plt
import torch.optim as optim
import copy
import pandas as pd

# @title
import tensorflow as tf
from tensorflow import keras
import os
import cv2
import numpy as np
import pandas as pd
import seaborn as sn
import tensorflow as tf
import matplotlib.pyplot as plt
import matplotlib.cm as cm
from google.colab import drive

from PIL import Image
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

!mkdir /root/.kaggle
# !cp /content/kaggle.json /content/drive/MyDrive
!cp '/content/drive/MyDrive/kaggle.json' /root/.kaggle

!unzip '/content/drive/MyDrive/affectnet.zip' -d '/content/Affectnet/'

import shutil

try:
    folder_path = '/content/Affectnet/neutral'
    shutil.rmtree(folder_path)
    print('Folder and its content removed') # Folder and its content removed
except:
    print('Folder not deleted')

import cv2
import numpy as np
from keras.utils import to_categorical
import os
import pandas as pd

def countFile(link):
    path = link
    num_files = len([f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))])

    print(f"Number of files in directory {link} is ", num_files)

countFile('/content/Affectnet/anger')
countFile('/content/Affectnet/contempt')
countFile('/content/Affectnet/disgust')
countFile('/content/Affectnet/fear')
countFile('/content/Affectnet/happy')
# countFile('/content/Affectnet/neutral')
countFile('/content/Affectnet/sad')
countFile('/content/Affectnet/surprise')

"""*Visualization to Number of Samples for the Affectnet Dataset*"""

import os
import matplotlib.pyplot as plt

# Define the directories for each emotion class
directories = [
    '/content/Affectnet/anger', '/content/Affectnet/contempt','/content/Affectnet/disgust',
    '/content/Affectnet/fear','/content/Affectnet/happy','/content/Affectnet/sad',
    '/content/Affectnet/surprise'
]

# Count the number of images in each directory
num_images = [len(os.listdir(directory)) for directory in directories]

# Define the emotion labels
emotions = ['Anger', 'Contempt','Disgust','Fear','Happy','Sad','Surprise']

# Define colors for each bar
colors = ['skyblue', 'salmon', 'lightcoral', 'lightgreen','blue', 'pink', 'lightgray', 'lightyellow']

# Create a bar plot with specified colors
plt.figure(figsize=(6, 4))
bars = plt.bar(emotions, num_images, color=colors)
plt.xlabel('Emotion Class')
plt.ylabel('Number of Images')
# plt.title('Number of Images per Emotion Class')
plt.xticks(rotation=45, ha="right")  # Rotate x-axis labels for better visibility

# Display the count of images on top of each bar
for bar, count in zip(bars, num_images):
    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 10, str(count), ha='center', fontsize=10)

# Adjust the layout and reduce the overall image size
plt.tight_layout()

# Show the plot
plt.show()

INPUT_PATH = '/content/Affectnet'
EMOTIONS = ["anger","contempt", "disgust","fear","happy", "sad", "surprise"]  # Update with your 4 emotions
IMAGE_SIZE = (96, 96)
classes = ["anger","contempt", "disgust","fear","happy","sad", "surprise"]

# EMOTIONS = ['Anger', 'Contempt','Disgust','Fear','Happiness','Sad','Surprise']

def image_generator(input_path, emotions, image_size):
    for label, emotion in enumerate(emotions):
        for filename in os.listdir(os.path.join(input_path, emotion)):
            img = cv2.imread(os.path.join(input_path, emotion, filename))
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB
            #img = cv2.resize(img, image_size)
            #img = img.astype('float32') / 255.0  # Normalize
            yield img, label

def load_images(input_path, emotions, image_size):
    X, y = [], []
    for img, label in image_generator(input_path, emotions, image_size):
        X.append(img)
        y.append(label)
    X = np.array(X)
    y = np.array(y)
    return X, y

X, y = load_images(INPUT_PATH, EMOTIONS, IMAGE_SIZE)
input_shape = X[0].shape
print(input_shape)

from sklearn.model_selection import train_test_split

# Split into train and test first (70% train, 30% test)
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42, stratify=y)

# Split the remaining data into validation and test (50% validation, 50% test)
x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=0.50, random_state=42, stratify=y_test)

# Now you have X_train, y_train, X_val, y_val, X_test, and y_test with proper shapes

y_train = to_categorical(y_train,dtype = 'int32')
y_val = to_categorical(y_val,dtype = 'int32')
y_test = to_categorical(y_test,dtype = 'int32')

print(x_train.shape)
print(x_val.shape)
print(x_test.shape)
print(y_train.shape)
print(y_val.shape)
print(y_test.shape)

"""*display a sample image*"""

import matplotlib.pyplot as plt
import numpy as np

# Example EMOTIONS list

EMOTIONS = ['Anger', 'Contempt','Disgust','Fear','Happiness','Sad','Surprise']
# Function to display a random image for each emotion class
def display_random_images_per_class(X, y, emotions):
    # Ensure y is a 2D array for one-hot encoded labels
    if y.ndim != 2:
        raise ValueError("y should be a 2D array for one-hot encoded labels (num_samples, num_classes)")

    num_classes = len(emotions)
    fig, axs = plt.subplots(1, num_classes, figsize=(num_classes * 3, 3))

    for i, emotion in enumerate(emotions):
        # Find all indices of images that belong to the current emotion class
        class_indices = np.where(np.argmax(y, axis=1) == i)[0]

        if len(class_indices) == 0:
            continue

        # Randomly select one index from these indices
        random_index = np.random.choice(class_indices)

        # Display the image and its corresponding label
        axs[i].imshow(X[random_index])
        axs[i].set_title(emotion)
        axs[i].axis('off')

    plt.tight_layout()
    plt.show()

# Ensure y_train is in one-hot encoded format
if y_train.ndim == 1 or y_train.shape[1] != len(EMOTIONS):
    raise ValueError("y_train should be a one-hot encoded 2D array")

# Debugging prints
print("x_train shape:", x_train.shape)
print("y_train shape:", y_train.shape)

# Example usage
display_random_images_per_class(x_train, y_train, EMOTIONS)

import matplotlib.pyplot as plt
import numpy as np



# Function to display a random image for each emotion class
def display_random_images_per_class(X, y, emotions, images_per_row=4):
    # Ensure y is a 2D array for one-hot encoded labels
    if y.ndim != 2:
        raise ValueError("y should be a 2D array for one-hot encoded labels (num_samples, num_classes)")

    num_classes = len(emotions)
    rows = (num_classes + images_per_row - 1) // images_per_row  # Calculate number of rows needed

    # Adjust figsize to reduce the size of the images
    fig, axs = plt.subplots(rows, images_per_row, figsize=(images_per_row * 2, rows * 2))

    for i, emotion in enumerate(emotions):
        # Find all indices of images that belong to the current emotion class
        class_indices = np.where(np.argmax(y, axis=1) == i)[0]

        if len(class_indices) == 0:
            continue

        # Randomly select one index from these indices
        random_index = np.random.choice(class_indices)

        # Calculate the position in the grid
        row = i // images_per_row
        col = i % images_per_row

        # Display the image and its corresponding label
        axs[row, col].imshow(X[random_index])
        axs[row, col].set_title(emotion)
        axs[row, col].axis('off')

    # Remove empty subplots
    for j in range(num_classes, rows * images_per_row):
        fig.delaxes(axs.flatten()[j])

    plt.tight_layout()
    plt.show()

# Ensure y_train is in one-hot encoded format
if y_train.ndim == 1 or y_train.shape[1] != len(EMOTIONS):
    raise ValueError("y_train should be a one-hot encoded 2D array")

# Debugging prints
print("x_train shape:", x_train.shape)
print("y_train shape:", y_train.shape)

# Example usage
display_random_images_per_class(x_train, y_train, EMOTIONS, images_per_row=4)

import torchvision.transforms as transforms
from torch.utils.data import Dataset

class CustomDataset(Dataset):
    def __init__(self, data, labels, transform=None):
        self.data = data
        self.labels = labels
        self.transform = transform

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        x = self.data[index]
        y = self.labels[index]
        if self.transform:
            x = self.transform(x)
        return x, y

train_dataset = CustomDataset(x_train, y_train, transform=transforms.Compose([
                            transforms.Resize((255, 255)),
                            transforms.ToTensor()
                        ]))

test_dataset = CustomDataset(x_test, y_test, transform=transforms.Compose([
                            transforms.Resize((255, 255)),
                            transforms.ToTensor()
                        ]))

val_dataset = CustomDataset(x_val, y_val, transform=transforms.Compose([
                            transforms.Resize((255, 255)),
                            transforms.ToTensor()
                        ]))

# import os
# import shutil
# import random

# data_dir = '/content/Affectnet'  # Replace with the actual path to your CKPlus dataset
# train_dir = '/content/train_dir'  # Replace with the path where you want to store the training set
# test_dir = '/content/test_dir'  # Replace with the path where you want to store the testing set
# test_ratio = 0.2  # The ratio of data to be used for testing
# # val_dir = '/content/val_dir'
# # Create the train and test directories if they don't exist
# os.makedirs(train_dir, exist_ok=True)
# os.makedirs(test_dir, exist_ok=True)

# # Iterate over the subfolders and randomly assign them to the train or test set
# for emo_folder in os.listdir(data_dir):
#     emo_path = os.path.join(data_dir, emo_folder)
#     if not os.path.isdir(emo_path):
#         continue
#     images = os.listdir(emo_path)
#     num_test = int(len(images) * test_ratio)
#     test_images = random.sample(images, num_test)
#     for img in images:
#         src = os.path.join(emo_path, img)
#         if img in test_images:
#             dst = os.path.join(test_dir, emo_folder, img)
#         else:
#             dst = os.path.join(train_dir, emo_folder, img)
#         os.makedirs(os.path.dirname(dst), exist_ok=True)
#         shutil.copy(src, dst)


#code with val_dir
import os
import shutil
import random

data_dir = '/content/Affectnet'  # Replace with the actual path to your dataset
train_dir = '/content/train_dir'  # Path to store the training set
val_dir = '/content/val_dir'      # Path to store the validation set
test_dir = '/content/test_dir'    # Path to store the testing set
test_ratio = 0.2  # Ratio of data to be used for testing
val_ratio = 0.1   # Ratio of data to be used for validation from the remaining training data

# Create the train, val, and test directories if they don't exist
os.makedirs(train_dir, exist_ok=True)
os.makedirs(val_dir, exist_ok=True)
os.makedirs(test_dir, exist_ok=True)

# Iterate over the subfolders and assign them to train, val, or test set
for emo_folder in os.listdir(data_dir):
    emo_path = os.path.join(data_dir, emo_folder)
    if not os.path.isdir(emo_path):
        continue
    images = os.listdir(emo_path)
    num_test = int(len(images) * test_ratio)
    num_val = int(len(images) * val_ratio)

    test_images = random.sample(images, num_test)
    remaining_images = list(set(images) - set(test_images))
    val_images = random.sample(remaining_images, num_val)
    train_images = list(set(remaining_images) - set(val_images))

    for img in images:
        src = os.path.join(emo_path, img)
        if img in test_images:
            dst = os.path.join(test_dir, emo_folder, img)
        elif img in val_images:
            dst = os.path.join(val_dir, emo_folder, img)
        else:
            dst = os.path.join(train_dir, emo_folder, img)
        os.makedirs(os.path.dirname(dst), exist_ok=True)
        shutil.copy(src, dst)

# %% Prepare data for pretrained model
train_dataset = torchvision.datasets.ImageFolder(
        root='/content/train_dir',
        transform=transforms.Compose([
                      transforms.Resize((255,255)),
                      transforms.ToTensor()
        ])
)

test_dataset = torchvision.datasets.ImageFolder(
        root='/content/test_dir',
        transform=transforms.Compose([
                      transforms.Resize((255,255)),
                      transforms.ToTensor()
        ])
)
val_dataset = torchvision.datasets.ImageFolder(
        root='/content/val_dir',
        transform=transforms.Compose([
                      transforms.Resize((255,255)),
                      transforms.ToTensor()
        ])
)

batch_size = 64
train_loader = torch.utils.data.DataLoader(
    train_dataset,
    batch_size=batch_size,
    shuffle=True
)
test_loader = torch.utils.data.DataLoader(
    test_dataset,
    batch_size=batch_size,
    shuffle=True
)
val_loader =  torch.utils.data.DataLoader(
    val_dataset,
    batch_size=batch_size,
    shuffle=True
)

# # from the test_loader to know how many samples are there.
# from collections import Counter

# # Initialize a Counter to keep track of the counts for each class
# class_counts = Counter()

# # Iterate through the test_loader
# for _, labels in test_loader:
#     # Update the Counter with the labels in the current batch
#     class_counts.update(labels.tolist())

# # Print the counts for each class
# for emotion_class, count in class_counts.items():
#     print(f"Class {emotion_class}: {count} samples")



from collections import Counter

EMOTIONS = ["anger", "contempt", "disgust", "fear", "happy", "sad", "surprise"]

# Initialize a Counter to keep track of the counts for each class
class_counts = Counter()

# Iterate through the test_loader
for _, labels in test_loader:
    # Update the Counter with the labels in the current batch
    class_counts.update(labels.tolist())

# Print the counts for each emotion
for class_label, count in class_counts.items():
    emotion = EMOTIONS[class_label]
    print(f"{emotion}: {count} samples")

"""*I am splitting the Code*"""

#This is the training, Validation and Testing the model
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from torchvision import models, transforms
import numpy as np

# Ensure the correct device (GPU or CPU) is used
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define the CNN model based on VGG16
class CNNModel(nn.Module):
    def __init__(self):
        super(CNNModel, self).__init__()
        self.vgg16 = models.vgg16(pretrained=True)

        # Replace output layer according to our problem
        in_feats = self.vgg16.classifier[6].in_features
        self.vgg16.classifier[6] = nn.Linear(in_feats, 7)  # Assuming 8 emotion classes

    def forward(self, x):
        x = self.vgg16(x)
        return x

# Instantiate the model and move it to the appropriate device
model = CNNModel().to(device)
model

# Know the shape of the test_loader
# Assuming you have already defined your test_loader

# Get a single batch from the test_loader
for images, labels in train_loader:
    print(f"Shape of images: {images.shape}")
    print(f"Shape of labels: {labels.shape}")
    break  # We only need to inspect one batch

for images, labels in test_loader:
    print(f"Shape of images: {images.shape}")
    print(f"Shape of labels: {labels.shape}")
    break  # We only need to inspect one batch

for images, labels in val_loader:
    print(f"Shape of images: {images.shape}")
    print(f"Shape of labels: {labels.shape}")
    break  # We only need to inspect one batch

# Training function
def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):
    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        for i, (inputs, labels) in enumerate(train_loader):
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item() * inputs.size(0)

            # Verbose output for each batch
            print(f"Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(train_loader)}, Loss: {loss.item():.4f}")

        # scheduler.step()

        epoch_loss = running_loss / len(train_loader.dataset)

        # Validation phase
        model.eval()
        val_loss = 0.0
        correct = 0
        total = 0
        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, labels)

                val_loss += loss.item() * inputs.size(0)
                _, preds = torch.max(outputs, 1)
                correct += (preds == labels).sum().item()
                total += labels.size(0)

        val_loss /= len(val_loader.dataset)
        val_acc = correct / total

        print(f"Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}")

criterion = nn.CrossEntropyLoss()
cross_entropy_loss = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.00002)

train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=50)

# Save the model to Google Drive
model_save_path = '/content/drive/My Drive/interrupted_model25epochs_1.pth'
torch.save(model.state_dict(), model_save_path)
print(f"Model saved to {model_save_path}")

# Save the model to Google Drive
model_save_path = '/content/drive/My Drive/interrupted_model1.pth'
torch.save(model.state_dict(), model_save_path)
print(f"Model saved to {model_save_path}")

# Testing the model accuracy
model.eval()  # Set the model to evaluation mode

test_loss = 0
correct = 0
total = 0

with torch.no_grad():
    for batch in test_loader:
        inputs, labels = batch
        inputs = inputs.to(device)
        labels = labels.to(device)

        outputs = model(inputs)
        _, predicted = torch.max(outputs.data, 1)

        total += labels.size(0)
        correct += (predicted == labels).sum().item()

        loss = cross_entropy_loss(outputs, labels)

        test_loss += loss.item()

    test_loss /= len(test_loader)
    accuracy = 100 * correct / total

    print(f"Test Loss: {test_loss:.4f}")
    print(f"Test Accuracy: {accuracy:.2f}%")

class_names = ['Anger', 'Contempt', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise']

#Code for confusion matrix generation
import torch
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Testing the model accuracy
model.eval()  # Set the model to evaluation mode

test_loss = 0
correct = 0
total = 0

all_labels = []
all_predictions = []

with torch.no_grad():
    for batch in test_loader:
        inputs, labels = batch
        inputs = inputs.to(device)
        labels = labels.to(device)

        outputs = model(inputs)
        _, predicted = torch.max(outputs.data, 1)

        total += labels.size(0)
        correct += (predicted == labels).sum().item()

        all_labels.extend(labels.cpu().numpy())
        all_predictions.extend(predicted.cpu().numpy())

        loss = cross_entropy_loss(outputs, labels)

        test_loss += loss.item()

    test_loss /= len(test_loader)
    accuracy = 100 * correct / total

    print(f"Test Loss: {test_loss:.4f}")
    print(f"Test Accuracy: {accuracy:.2f}%")

# Compute confusion matrix
cm = confusion_matrix(all_labels, all_predictions)

# Plot confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=class_names, yticklabels=class_names)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

"""11 - 6 -2024 Evening Newly trying after github

**Relevance Calculation**
"""

def new_layer(layer, g):
    """Clone a layer and pass its parameters through the function g."""
    layer = copy.deepcopy(layer)
    try: layer.weight = torch.nn.Parameter(g(layer.weight))
    except AttributeError: pass
    try: layer.bias = torch.nn.Parameter(g(layer.bias))
    except AttributeError: pass
    return layer

def dense_to_conv(layers):
    """ Converts a dense layer to a conv layer """
    newlayers = []
    for i,layer in enumerate(layers):
        if isinstance(layer, nn.Linear):
            newlayer = None
            if i == 0:
                m, n = 512, layer.weight.shape[0]
                newlayer = nn.Conv2d(m,n,7)
                newlayer.weight = nn.Parameter(layer.weight.reshape(n,m,7,7))
            else:
                m,n = layer.weight.shape[1],layer.weight.shape[0]
                newlayer = nn.Conv2d(m,n,1)
                newlayer.weight = nn.Parameter(layer.weight.reshape(n,m,1,1))
            newlayer.bias = nn.Parameter(layer.bias)
            newlayers += [newlayer]
        else:
            newlayers += [layer]
    return newlayers

def get_linear_layer_indices(model):
    offset = len(model.vgg16._modules['features']) + 1
    indices = []
    for i, layer in enumerate(model.vgg16._modules['classifier']):
        if isinstance(layer, nn.Linear):
            indices.append(i)
    indices = [offset + val for val in indices]
    return indices

def apply_lrp_on_vgg16(model, image):
    image = torch.unsqueeze(image, 0)
    # >>> Step 1: Extract layers
    layers = list(model.vgg16._modules['features']) \
                + [model.vgg16._modules['avgpool']] \
                + dense_to_conv(list(model.vgg16._modules['classifier']))
    linear_layer_indices = get_linear_layer_indices(model)
    # >>> Step 2: Propagate image through layers and store activations
    n_layers = len(layers)
    activations = [image] + [None] * n_layers # list of activations

    for layer in range(n_layers):
        if layer in linear_layer_indices:
            if layer == 32:
                activations[layer] = activations[layer].reshape((1, 512, 7, 7))
        activation = layers[layer].forward(activations[layer])
        if isinstance(layers[layer], torch.nn.modules.pooling.AdaptiveAvgPool2d):
            activation = torch.flatten(activation, start_dim=1)
        activations[layer+1] = activation

    # >>> Step 3: Replace last layer with one-hot-encoding
    output_activation = activations[-1].detach().cpu().numpy()
    max_activation = output_activation.max()
    one_hot_output = [val if val == max_activation else 0
                        for val in output_activation[0]]

    activations[-1] = torch.FloatTensor([one_hot_output]).to(device)

    # >>> Step 4: Backpropagate relevance scores
    relevances = [None] * n_layers + [activations[-1]]
    # Iterate over the layers in reverse order
    for layer in range(0, n_layers)[::-1]:
        current = layers[layer]
        # Treat max pooling layers as avg pooling
        if isinstance(current, torch.nn.MaxPool2d):
            layers[layer] = torch.nn.AvgPool2d(2)
            current = layers[layer]
        if isinstance(current, torch.nn.Conv2d) or \
           isinstance(current, torch.nn.AvgPool2d) or\
           isinstance(current, torch.nn.Linear):
            activations[layer] = activations[layer].data.requires_grad_(True)

            # Apply variants of LRP depending on the depth
            # see: https://link.springer.com/chapter/10.1007%2F978-3-030-28954-6_10
            # Lower layers, LRP-gamma >> Favor positive contributions (activations)
            if layer <= 16:       rho = lambda p: p + 0.25*p.clamp(min=0); incr = lambda z: z+1e-9
            # Middle layers, LRP-epsilon >> Remove some noise / Only most salient factors survive
            if 17 <= layer <= 30: rho = lambda p: p;                       incr = lambda z: z+1e-9+0.25*((z**2).mean()**.5).data
            # Upper Layers, LRP-0 >> Basic rule
            if layer >= 31:       rho = lambda p: p;                       incr = lambda z: z+1e-9

            # Transform weights of layer and execute forward pass
            z = incr(new_layer(layers[layer],rho).forward(activations[layer]))
            # Element-wise division between relevance of the next layer and z
            s = (relevances[layer+1]/z).data
            # Calculate the gradient and multiply it by the activation
            (z * s).sum().backward();
            c = activations[layer].grad
            # Assign new relevance values
            relevances[layer] = (activations[layer]*c).data
        else:
            relevances[layer] = relevances[layer+1]

    # >>> Potential Step 5: Apply different propagation rule for pixels
    return relevances[0]

# %% Train
cross_entropy_loss = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.00002)
epochs = 50

import torch

# Load the saved model
# model = CNNModel()  # Replace YourModelClass with the actual class of your model
# model.load_state_dict(torch.load("/content/drive/MyDrive/CK+_RAF_model_50Epo.pth"))
model.eval()  # Set the model to evaluation mode

# Rest of your testing code
test_loss = 0
correct = 0
total = 0

with torch.no_grad():
    for batch in test_loader:
        inputs, labels = batch
        inputs = inputs.to(device)
        labels = labels.to(device)

        outputs = model(inputs)
        _, predicted = torch.max(outputs.data, 1)

        total += labels.size(0)
        correct += (predicted == labels).sum().item()

        loss = cross_entropy_loss(outputs, labels)
        test_loss += loss.item()

    test_loss /= len(test_loader)
    accuracy = 100 * correct / total

    print(f"Test Loss: {test_loss:.4f}")
    print(f"Test Accuracy: {accuracy:.2f}%")



def get_pixel_count_in_range(array):
  array = array.flatten()
  count_range = []
  for i in range(10):
    x, y = i/10, (i+1)/10
    selected_values = array[np.logical_and(array > x, array <= y)]
    #print(selected_values)
    # Count the number of values in the range
    # count = np.count_nonzero(np.logical_and(array > x, array <= y))
    count = len(selected_values)
    # Print the count
    print("Contribution of pixels in the range [",x,y,"] is : ", count)
    count_range.append(count)
  return count_range

def calculate_range_by_mean5(array):
  pixel_count_in_range = get_pixel_count_in_range(array)
  m1 = sum(pixel_count_in_range[:5])
  m2 = sum(pixel_count_in_range[3:8])
  m3 = sum(pixel_count_in_range[5:])
  print("all three means",'m1=',m1,'m2=',m2,'m3=', m3)
  print(max(m1,m2,m3))
  lmh = ["Minimal", "Average", "Strong"]
  lmhv = [m1, m2, m3]

  intensity = lmh[lmhv.index(max(lmhv))]
  # print(intensity)
  return intensity

n = 1
image_names = []
positive = []
negative_counts = []
relevance_scores = []

for inputs, labels in test_loader:
    inputs = inputs.to(device)
    outputs = model(inputs).max(1).indices.detach().cpu().numpy()
    for i in range(inputs.shape[0]):
        image_id = i

        try:
            image_relevances = apply_lrp_on_vgg16(model, inputs[image_id])
            image_relevances = image_relevances.permute(0, 2, 3, 1).detach().cpu().numpy()[0]
            image_relevances = np.interp(image_relevances, (image_relevances.min(), image_relevances.max()), (0, 1))
            image_relevances_transposed = np.transpose(image_relevances, (2, 0, 1))
            pixel_relevance_scores = inputs[image_id].detach().cpu().numpy() * np.transpose(image_relevances, (2, 0, 1))

            pred_label = list(train_dataset.class_to_idx.keys())[list(train_dataset.class_to_idx.values()).index(labels[image_id])]
            image_name = pred_label + '_' + str(n)
            n += 1
            # Add condition to process images only for the specified emotion class
            # if pred_label == emotion_class:
            if outputs[image_id] == labels[image_id]:
                print("Groundtruth for this image:", pred_label)
                plt.clf()  # Clear the previous figure

                plt.subplot(1, 5, 1)
                plt.imshow(inputs[image_id].permute(1, 2, 0).detach().cpu().numpy())
                plt.title(image_name)

                plt.subplot(1, 5, 2)
                plt.imshow(image_relevances[:, :, 0], cmap="seismic")
                plt.title("Image Relevance")

                plt.subplot(1, 5, 3)
                plt.imshow(pixel_relevance_scores[0], cmap="seismic")
                plt.title("Channel 1")

                plt.subplot(1, 5, 4)
                plt.imshow(pixel_relevance_scores[1], cmap="seismic")
                plt.title("Channel 2")

                plt.subplot(1, 5, 5)
                plt.imshow(pixel_relevance_scores[2], cmap="seismic")
                plt.title("Channel 3")

                plt.tight_layout()
                plt.show()

                # p, z = contributed_pixels_count(1, pixel_relevance_scores)
                image_names.append(image_name)
                # positive_counts.append(p)
                # negative_counts.append(z)
                relevance_scores.append(image_relevances)
                print("-------------------------------------------------------------------------------------------------------")
                # print((image_relevances.shape))
                intensity = calculate_range_by_mean5(image_relevances)
                print(intensity+" "+pred_label)
                print("-------------------------------------------------------------------------------------------------------")

        except TypeError:
            continue

# my modification

n = 1
image_names = []
positive = []
negative_counts = []
relevance_scores = []

for inputs, labels in test_loader:
    inputs = inputs.to(device)
    outputs = model(inputs).max(1).indices.detach().cpu().numpy()
    for i in range(inputs.shape[0]):
        image_id = i

        try:
            image_relevances = apply_lrp_on_vgg16(model, inputs[image_id])
            image_relevances = image_relevances.permute(0, 2, 3, 1).detach().cpu().numpy()[0]
            image_relevances = np.interp(image_relevances, (image_relevances.min(), image_relevances.max()), (0, 1))
            image_relevances_transposed = np.transpose(image_relevances, (2, 0, 1))
            pixel_relevance_scores = inputs[image_id].detach().cpu().numpy() * np.transpose(image_relevances, (2, 0, 1))

            pred_label = list(train_dataset.class_to_idx.keys())[list(train_dataset.class_to_idx.values()).index(labels[image_id])]
            image_name = pred_label + '_' + str(n)
            n += 1
            # Add condition to process images only for the specified emotion class
            # if pred_label == emotion_class:
            if outputs[image_id] == labels[image_id]:
                print("Groundtruth for this image:", pred_label)
                plt.clf()  # Clear the previous figure

                plt.subplot(1, 5, 1)
                plt.imshow(inputs[image_id].permute(1, 2, 0).detach().cpu().numpy())
                plt.title(image_name)

                plt.subplot(1, 5, 2)
                plt.imshow(image_relevances[:, :, 0], cmap="seismic")
                plt.title("Image Relevance")

                plt.subplot(1, 5, 3)
                plt.imshow(pixel_relevance_scores[0], cmap="seismic")
                plt.title("Channel 1")

                plt.subplot(1, 5, 4)
                plt.imshow(pixel_relevance_scores[1], cmap="seismic")
                plt.title("Channel 2")

                plt.subplot(1, 5, 5)
                plt.imshow(pixel_relevance_scores[2], cmap="seismic")
                plt.title("Channel 3")

                plt.tight_layout()
                plt.show()

                # p, z = contributed_pixels_count(1, pixel_relevance_scores)
                image_names.append(image_name)
                # positive_counts.append(p)
                # negative_counts.append(z)
                relevance_scores.append(image_relevances)
                print("-------------------------------------------------------------------------------------------------------")
                # print((image_relevances.shape))
                intensity = calculate_range_by_mean5(image_relevances)
                print(intensity+" "+pred_label)
                print("-------------------------------------------------------------------------------------------------------")

        except TypeError:
            continue

# emotion_class = 'happy'  # Specify the emotion class for which you want to generate relevance scores

folder_path = '/content/drive/MyDrive/hm'
n = 1
image_names = []
positive = []
negative_counts = []
relevance_scores = []

for inputs, labels in train_loader:
    inputs = inputs.to(device)
    outputs = model(inputs).max(1).indices.detach().cpu().numpy()
    for i in range(inputs.shape[0]):
        image_id = i

        try:
            image_relevances = apply_lrp_on_vgg16(model, inputs[image_id])
            image_relevances = image_relevances.permute(0, 2, 3, 1).detach().cpu().numpy()[0]
            image_relevances = np.interp(image_relevances, (image_relevances.min(), image_relevances.max()), (0, 1))
            image_relevances_transposed = np.transpose(image_relevances, (2, 0, 1))
            pixel_relevance_scores = inputs[image_id].detach().cpu().numpy() * np.transpose(image_relevances, (2, 0, 1))

            pred_label = list(train_dataset.class_to_idx.keys())[list(train_dataset.class_to_idx.values()).index(labels[image_id])]
            image_name = pred_label + '_' + str(n)
            n += 1
            # Add condition to process images only for the specified emotion class
            # if pred_label == emotion_class:
            if outputs[image_id] == labels[image_id]:
                print("Groundtruth for this image:", pred_label)
                plt.clf()  # Clear the previous figure

                plt.subplot(1, 5, 1)
                plt.imshow(inputs[image_id].permute(1, 2, 0).detach().cpu().numpy())
                plt.title(image_name)

                plt.subplot(1, 5, 2)
                plt.imshow(image_relevances[:, :, 0], cmap="seismic")
                plt.title("Image Relevance")

                plt.subplot(1, 5, 3)
                plt.imshow(pixel_relevance_scores[0], cmap="seismic")
                plt.title("Channel 1")

                plt.subplot(1, 5, 4)
                plt.imshow(pixel_relevance_scores[1], cmap="seismic")
                plt.title("Channel 2")

                plt.subplot(1, 5, 5)
                plt.imshow(pixel_relevance_scores[2], cmap="seismic")
                plt.title("Channel 3")

                plt.tight_layout()
                plt.show()

                # p, z = contributed_pixels_count(1, pixel_relevance_scores)
                image_names.append(image_name)
                # positive_counts.append(p)
                # negative_counts.append(z)
                relevance_scores.append(image_relevances)
                print("-------------------------------------------------------------------------------------------------------")
                # print((image_relevances.shape))
                intensity = calculate_range_by_mean5(image_relevances)
                print(intensity+" "+pred_label)
                print("-------------------------------------------------------------------------------------------------------")

        except TypeError:
            continue

"""#Only for a particular  *emotion*"""

import torch
import numpy as np
import matplotlib.pyplot as plt

n = 1
image_names = []
positive_counts = []
negative_counts = []
relevance_scores = []

# Set the emotion class to filter
emotion_class = 'fear'  # change the emotion name all small letters.

for inputs, labels in test_loader:
    inputs = inputs.to(device)
    outputs = model(inputs).max(1).indices.detach().cpu().numpy()

    for i in range(inputs.shape[0]):
        image_id = i
        pred_label = list(train_dataset.class_to_idx.keys())[list(train_dataset.class_to_idx.values()).index(labels[image_id])]

        # Process only if the predicted label is the specified emotion class
        if pred_label == emotion_class:
            try:
                image_relevances = apply_lrp_on_vgg16(model, inputs[image_id])
                image_relevances = image_relevances.permute(0, 2, 3, 1).detach().cpu().numpy()[0]
                image_relevances = np.interp(image_relevances, (image_relevances.min(), image_relevances.max()), (0, 1))
                image_relevances_transposed = np.transpose(image_relevances, (2, 0, 1))
                pixel_relevance_scores = inputs[image_id].detach().cpu().numpy() * image_relevances_transposed

                image_name = pred_label + '_' + str(n)
                n += 1

                if outputs[image_id] == labels[image_id]:
                    print("Groundtruth for this image:", pred_label)
                    plt.clf()  # Clear the previous figure

                    plt.subplot(1, 5, 1)
                    plt.imshow(inputs[image_id].permute(1, 2, 0).detach().cpu().numpy())
                    plt.title(image_name)

                    plt.subplot(1, 5, 2)
                    plt.imshow(image_relevances[:, :, 0], cmap="seismic")
                    plt.title("Image Relevance")

                    plt.subplot(1, 5, 3)
                    plt.imshow(pixel_relevance_scores[0], cmap="seismic")
                    plt.title("Channel 1")

                    plt.subplot(1, 5, 4)
                    plt.imshow(pixel_relevance_scores[1], cmap="seismic")
                    plt.title("Channel 2")

                    plt.subplot(1, 5, 5)
                    plt.imshow(pixel_relevance_scores[2], cmap="seismic")
                    plt.title("Channel 3")

                    plt.tight_layout()
                    plt.show()

                    image_names.append(image_name)
                    relevance_scores.append(image_relevances)
                    print("-------------------------------------------------------------------------------------------------------")
                    intensity = calculate_range_by_mean5(image_relevances)
                    print(intensity + " " + pred_label)
                    print("-------------------------------------------------------------------------------------------------------")

            except TypeError:
                continue

"""# code to be worked on one image from x_test dataset."""